#!/usr/bin/env python
# encoding: utf-8
'''
MAGAL -- Magnitudes Analyzer fitting program.

@author:     william
        
@license:    GPLv3

@contact:    william@iaa.es
'''

import sys
import os

import logging
import magal.core.log

from argparse import ArgumentParser
from argparse import RawDescriptionHelpFormatter

from magal.core.version import _magal_version_, _magal_updated_
from magal.core.exceptions import MAGALCLIError

import h5py
import numpy as np

from magal.io.readlibrary import Library
from magal.fit.stats import chi2, percentiles

from magal.io.readinput import Input
from magal.util.matchs import get_zslice
import ConfigParser
from ConfigParser import NoSectionError, NoOptionError
import ast

DEBUG = 0
TESTRUN = 0
PROFILE = 0

    

def main(argv=None): # IGNORE:C0111
    '''Command line options.'''
    
    log = logging.getLogger('magal.main')
    
    if argv is None:
        argv = sys.argv
    else:
        sys.argv.extend(argv)

    program_name = os.path.basename(sys.argv[0])
    program_version = "v%s" % _magal_version_
    program_build_date = str(_magal_updated_)
    program_version_message = '%%(prog)s %s (%s)' % (program_version, program_build_date)
    program_shortdesc = __import__('__main__').__doc__.split("\n")[1]
    program_license = '''%s

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

USAGE
''' % (program_shortdesc)

    try:
        # Setup argument parser
        parser = ArgumentParser(description=program_license, formatter_class=RawDescriptionHelpFormatter)
        
        parser.add_argument("-i", "--inputfile", dest="input", help="Input configuration", required=True)
#         parser.add_argument("-i", "--inputfile", dest="input", help="Input filename ", required=True)
#         parser.add_argument("-l", "--library", dest="Library", help="Template Library filename ", required=True)
#         parser.add_argument("-f", "--filtersystem", dest="filtersystem", help="Filtersystem ", required=True)
#         parser.add_argument("-c", "--ccd", dest="ccd", help="CCD ", required=True)
#         parser.add_argument("-o", "--outputfile", dest="output", help="Output filename ", required=True)
#         parser.add_argument("-s", "--simulate", dest="sim", action="store_true", default=False, help="If enabled, will treat as a simulation. Inputfile must be a template library.")
#         parser.add_argument("-z", "--obj_z", default=None, dest="obj_z", type=float, help="Redshift to get on the inputfile library. To use with -s.")
#         parser.add_argument("-m", "--mass_field", default=None, dest="mass_field", type=str, help="Field corresponding to the log10(Mass) on inputfile property table.")
#         
#         parser.add_argument("-N", "--Ngals_max", dest="Nmax", type=int, help="Max number of input galaxies to run over")
#                 
#         parser.add_argument("-nz", "--no_z", dest="nz", action="store_true", default=False, help="Evaluate at z = const. Redshift will be get from inputfile property z.")
#         
        parser.add_argument("-v", "--verbose", dest="verbose", action="count", help="set verbosity level [default: %(default)s]")
        parser.add_argument('-V', '--version', action='version', version=program_version_message)
        
        # Process arguments
        args = parser.parse_args()
        config = ConfigParser.ConfigParser()
        config.read(args.input) 
#        TODO: Sanity checks here!!!
        
        try:
            if(config.getboolean('FitSimulation', 'is_simulation')):     #Simulation-related sanity checks
                try: 
                    log.info('Using z = %s from inputfile on this simulation.' % config.getfloat('FitSimulation', 'obj_z'))
                except NoOptionError:
                    log.info('Using z from inputfile tables.')
#                 try:
#                     log.info('Using mass from inputfile table field %s.' % config.get('FitSimulation', 'mass_field'))
#                 except NoOptionError:
#                     pass
#                 if args.nz:
#                     raise MAGALCLIError('Error! You cannot have -s and -nz options at the same time.')
            else:
                is_simulation = False #This is not a simulation. Skip all the simulation-related parts.
        except NoSectionError:
            is_simulation = False #This is not a simulation. Skip all the simulation-related parts.
     
        
        if args.verbose > 0:
            log.setLevel('DEBUG')
            
    except KeyboardInterrupt:
        print 'CTRL+C pressed... exiting...' #TODO: move this to the logger.
        return 0
    
    except Exception, e:
        if DEBUG or TESTRUN:
            raise(e)
        indent = len(program_name) * " "
        sys.stderr.write(program_name + ": " + repr(e) + "\n")
        sys.stderr.write(indent + "  for help use --help")
        return 2
    

    # 1 - Load files
    # 1.1 - Inputfile
    if(is_simulation): # If this is a simulation, a Library will be the inputfile. 
        inp = Library(config.get('FitGeneral', 'mag_file'))
        inp.get_filtersys(config.get('FitGeneral', 'filter_sys'), config.get('FitGeneral', 'ccd'))
        #TODO: multiply here by args.mass
        magal_type = 'simulation'
    else:
        inp = Input(config.get('FitGeneral', 'mag_file'))
        try:
            inp.get_filtersys(config.get('FitGeneral', 'filter_sys'), config.get('FitGeneral', 'ccd'))
        except KeyError:
            raise MAGALCLIError('Data not found! Are you sure that you do not want to Simulate?')
        magal_type = 'data'
    
    # 1.2 - Library
    lib = Library(config.get('FitGeneral', 'lib_file'))
    lib.get_filtersys(config.get('FitGeneral', 'filter_sys'), config.get('FitGeneral', 'ccd')) # libra
    
    # 1.3 - Get objectlist
    if(is_simulation):
        o_list = get_zslice(inp, config.getfloat('FitSimulation', 'obj_z'))
    else:
        o_list = inp.data
        
    try:
        if config.getint('FitGeneral', 'Nobj_max') > 0:
            o_list = o_list[:config.getint('FitGeneral', 'Nobj_max')]
            if (args.verbose): print 'Running magal fit to ONLY %i objects.' % config.getint('FitGeneral', 'Nobj_max') 
        elif (args.verbose): print 'Running magal fit to ALL objects.' 
    except NoOptionError:
        if (args.verbose): print 'Running magal fit to ALL objects.'
    
    N_obj = len(o_list)
    
    # 2 - Output file
    # 2.1 - init 
    try:
        f = h5py.File(config.get('FitGeneral', 'output_file'), mode = 'w-')
    except IOError:
        raise MAGALCLIError('File %s already exists.' % config.get('FitGeneral', 'output_file'))
    
    # 2.2 - Define some auxiliar data...
    f.attrs.create('ifile', config.get('FitGeneral', 'mag_file'))
    f.attrs.create('lib', config.get('FitGeneral', 'lib_file'))
    f.attrs.create('type', magal_type)
    f.attrs.create('version', '%s - %s' % (program_name, program_version))
    
    # Write full .ini file on model outputfile.
    aux_ini = open(args.input, 'r').read()
    str_type = h5py.new_vlen(str)
    ds = f.create_dataset('/ini_file', shape=(1,), dtype=str_type)
    ds[:] = aux_ini
    ### 

    # 2.3 - Data matrices
    # 2.3.1 - Shape is defined by (N_obj, N_z, N_lib)
    if(config.getboolean('FitGeneral', 'Nz')):
        aux_shape = (N_obj, 1, lib.library.shape[1])
    else:
        aux_shape = (N_obj, lib.library.shape[0], lib.library.shape[1])
    
    # 2.3.2 - Create datasets
    n_ds = f.create_dataset( '%s/n' % (lib.path), shape = aux_shape, dtype = np.int) # Number of used pixels on \chi2 calc
    s_ds = f.create_dataset( '%s/s' % (lib.path), shape = aux_shape) # Scaling-factor
    chi2_ds = f.create_dataset( '%s/chi2' % (lib.path), aux_shape) # \Chi2
    mod_stats_grp = f.create_group( '%s/statistics/model' % (lib.path)) # Model likelihood statistics (i.e. average, percentiles, etc..)
    lib_stats_grp = f.create_group( '%s/statistics/library' % (lib.path)) # Library likelihood statistics (i.e. average, percentiles, etc..)
    
    
    
    # 3 - Fit
    
    if config.getboolean('FitGeneral', 'Nz'):
        N_z, N_tpl = (1, lib.library.shape[1]) # If the redshift comes from inputfile, N_z = 1.
        inp_z = inp.z #properties['z']
    else:
        N_z, N_tpl = lib.library.shape[:2]
        inp_z = lib.z
    
    try:
        filters_include = ast.literal_eval(config.get('FitGeneral', 'filters_include'))
        filterset_mask = np.sum([k == lib.filterset['ID_filter'] for k in filters_include], axis=0, dtype=np.bool)
    except NoOptionError:
        filterset_mask = np.ones(len(lib.filterset), dtype=np.bool)
    
    # 3.3 - Eval \chi^2
    for i_obj in range(N_obj):
        obj = o_list[i_obj]        
        for i_z in range(N_z):
            if config.getboolean('FitGeneral', 'Nz'): # If redshift comes from inputfile.
                a = get_zslice(lib, inp_z[i_obj])
                log.debug('Redshift slicing inp_z, lib_z: %3.4f, %3.4f' % (inp_z[i_obj], lib.z[np.argmin((lib.z - inp_z[i_obj])**2)]) )
            else:
                a = get_zslice(lib, inp_z[i_z])
            for i_tpl in range(N_tpl):
                # 3.2 - If this is a simulation, use 1% of magnitude as error.
                if(is_simulation):
                    w = 1 / (obj['e_ab'][filterset_mask] + np.random.normal(size=len(obj['m_ab'][filterset_mask]), scale=0.05)) #TODO: Put a more realistic error component.
                else:
                    w = 1 / obj['e_ab'][filterset_mask]
                
                n_ds[i_obj,i_z,i_tpl], s_ds[i_obj,i_z,i_tpl], chi2_ds[i_obj,i_z,i_tpl] = chi2(obj['m_ab'][filterset_mask], a[i_tpl]['m_ab'][filterset_mask], w)
                if (i_z % 5 == 0 or config.getboolean('FitGeneral', 'Nz')) and i_tpl == 0 and args.verbose > 0:
                    log.setLevel('DEBUG')
                    log.debug('I\'m at i_obj, i_z --> %s, %s' % (i_obj, i_z))
                if args.verbose > 0 and i_tpl == 0:    
                    print 'I\'m at i_obj, i_z --> %s, %s' % (i_obj, i_z)
    
    # 4 - From \chi^2, eval likelihood
    # 4.1 - Likelihood
    log.debug('Calculating Likelihood')
    l = np.exp( -0.5 * ( np.subtract(chi2_ds, np.min(chi2_ds, axis=2).reshape(N_obj, N_z, 1)) ) )
    # 4.2 - Normalization
    l /= np.sum(l, axis=2).reshape(N_obj, N_z, 1) #FIXME: Double sum here!
    likelihood_ds = f.create_dataset( '%s/likelihood' % (lib.path), data = l )
    l_T = np.sum(l, axis=1)
    norm = np.sum(l_T, axis=1)
    l_T = np.array([l_T[i_obj] / norm[i_obj] for i_obj in range(N_obj)])
    lT_ds = f.create_dataset( '%s/likelihood_template' % (lib.path), data = l_T )  # @UnusedVariable
    if N_z > 1:
        l_z = np.sum(l, axis=2)
        norm = np.sum(l_z, axis=1)
        l_z = np.array([l_z[i_obj] / norm[i_obj] for i_obj in range(N_obj)])
        lz_ds = f.create_dataset( '%s/likelihood_redshift' % (lib.path), data = np.array(l_z) )  # @UnusedVariable
    
    # 5 - From likelihood, eval statistics...
    # 5.0 - Some definitions...
    perc = [2.5, 16, 50, 84, 97.5] # Percentiles which we calculate.
    perc_dt = np.dtype([ ('%s' % p, np.float) for p in perc ])
    
    # 5.1 - Model statistics
    # 5.1.1 - Scaling-factor "s".
    log.debug('Calculating scaling-factor statistics')
    p_grp = mod_stats_grp.create_group('s_Mass') # Scale prop group
    s_Mass = np.divide(s_ds.value.copy(),-2.5)
    
    # 5.1.2 - Best match
    log.debug('Calculating Best Matches')
    bmx_ds = mod_stats_grp.create_dataset('i_BMX', shape=(N_obj, 2), dtype=np.int) # Store which template is the bestmatch. [i_z, i_tpl]
    for i_obj in range(N_obj):
        bmx_flat = np.argmax(np.ravel(l[i_obj]))
        bmx_ds[i_obj,0] = bmx_flat / N_tpl  # i_z bmx
        bmx_ds[i_obj,1] = bmx_flat % N_tpl  # i_tpl bmx
    
    # 5.1.3 - Percentiles
    log.debug('Calculating Mass percentiles')
    p_ds = p_grp.create_dataset('percentiles', shape=(N_obj,), dtype=perc_dt) # Percentiles dataset 
    for i_obj in range(N_obj):
        p_ds[i_obj] = tuple(percentiles(np.ravel(s_Mass[i_obj]), np.ravel(likelihood_ds[i_obj]), perc)) 
        
    # 5.1.4 - Average
    log.debug('Calculating Mass average')
    p_ds = p_grp.create_dataset('AVG', shape=(N_obj,))
    for i_obj in range(N_obj): p_ds[i_obj] = np.sum(s_Mass[i_obj] * likelihood_ds[i_obj])
    
    # 5.2 - Library properties statistics
    for prop in lib.properties.dtype.names:
        if lib.properties.dtype[prop].kind == 'f': # Do statistics only for float properties. 
            p_grp = lib_stats_grp.create_group(prop) # Create a group for each prop
            
            # 5.2.2 - Percentiles
            log.debug('Calculating property %s percentiles' % prop)
            p_ds = p_grp.create_dataset('percentiles', shape=(N_obj,), dtype=perc_dt) # Percentiles dataset
            for i_obj in range(N_obj):
                p_ds[i_obj] = tuple(percentiles(lib.properties[prop], l_T[i_obj], perc))
                
            # 5.2.3 - Average
            log.debug('Calculating property %s average' % prop)
            p_grp.create_dataset('AVG', data = np.sum(l_T * lib.properties[prop], axis=1) / np.sum(l_T, axis=1))
            
            # 5.2.4 - -999 probability (or NaN)
            if np.isnan(lib.properties[prop]).sum() > 0: # Will do this only if library property have NaNs.
                log.debug('Calculating property %s NaN probability' % prop)
                p_grp.create_dataset('pNaN', aux_shape) #TODO: Add data to NaN properties.

    f.close()
    
    log.debug('Finished.')

if __name__ == "__main__":
    if DEBUG:
        sys.argv.append("-v")
    if TESTRUN:
        import doctest
        doctest.testmod()
    if PROFILE:
        import cProfile
        import pstats
        profile_filename = 'scripts.magal.profile'
        cProfile.run('main()', profile_filename)
        statsfile = open("profile_stats.txt", "wb")
        p = pstats.Stats(profile_filename, stream=statsfile)
        stats = p.strip_dirs().sort_stats('cumulative')
        stats.print_stats()
        statsfile.close()
        sys.exit(0)
    sys.exit(main())